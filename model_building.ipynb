{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "from preprocessing import *\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load vectors of word indices representing the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_embedding(token_train, token_test, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_train = pd.read_pickle(\"./data/word2vec_train.pkl\")\n",
    "w2v_test = pd.read_pickle(\"./data/word2vec_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>tokenq1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>tokenq2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 2, 3, 4, 5, 4, 6, 7, 8, 9, 10, 8, 11]</td>\n",
       "      <td>241804</td>\n",
       "      <td>[1, 2, 3, 4, 5, 4, 6, 7, 8, 9, 10]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 3, 12, 13, 13, 14]</td>\n",
       "      <td>13483</td>\n",
       "      <td>[1, 15, 16, 17, 3, 18, 19, 20, 3, 13, 13, 14, 21]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[22, 23, 24, 25, 3, 26, 27, 28, 29, 30, 31, 32]</td>\n",
       "      <td>241805</td>\n",
       "      <td>[22, 23, 28, 26, 33, 34, 5, 35, 36, 37]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[38, 39, 24, 40, 41, 42, 22, 23, 24, 43, 44]</td>\n",
       "      <td>241806</td>\n",
       "      <td>[45, 3, 46, 47, 48, 49, 50, 49, 51, 48, 2, 52,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[53, 54, 55, 8, 56, 57, 58, 59, 60, 61, 62, 63]</td>\n",
       "      <td>241807</td>\n",
       "      <td>[53, 64, 15, 65, 8, 59, 56]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1                                          tokenq1    qid2  \\\n",
       "0   0     0        [1, 2, 3, 4, 5, 4, 6, 7, 8, 9, 10, 8, 11]  241804   \n",
       "1   1     1                        [1, 2, 3, 12, 13, 13, 14]   13483   \n",
       "2   2     2  [22, 23, 24, 25, 3, 26, 27, 28, 29, 30, 31, 32]  241805   \n",
       "3   3     3     [38, 39, 24, 40, 41, 42, 22, 23, 24, 43, 44]  241806   \n",
       "4   4     4  [53, 54, 55, 8, 56, 57, 58, 59, 60, 61, 62, 63]  241807   \n",
       "\n",
       "                                             tokenq2  is_duplicate  \n",
       "0                 [1, 2, 3, 4, 5, 4, 6, 7, 8, 9, 10]             0  \n",
       "1  [1, 15, 16, 17, 3, 18, 19, 20, 3, 13, 13, 14, 21]             0  \n",
       "2            [22, 23, 28, 26, 33, 34, 5, 35, 36, 37]             0  \n",
       "3  [45, 3, 46, 47, 48, 49, 50, 49, 51, 48, 2, 52,...             0  \n",
       "4                        [53, 64, 15, 65, 8, 59, 56]             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Google News vectors\n",
    "googlenews_embedding = './data/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(googlenews_embedding, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f7a0f029d119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build the embedding matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# This will be the embedding matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# So that the padding will be ignored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "# Build the embedding matrix\n",
    "embedding_dim = 300\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']\n",
    "stops = set(stopwords.words('english'))\n",
    "questions = ['tokenq1', 'tokenq2']\n",
    "\n",
    "# Iterate over the questions of both training and test datasets\n",
    "for dataset in [token_train, token_test]:\n",
    "    for index, row in dataset.iterrows():\n",
    "\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for question in questions:\n",
    "\n",
    "            question_to_number = []  # q2n -> question numbers representation\n",
    "            for word in row[question]:\n",
    "\n",
    "                # Check for unwanted words\n",
    "                if word in stops and word not in word2vec.vocab:\n",
    "                    continue\n",
    "\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    question_to_number.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                else:\n",
    "                    question_to_number.append(vocabulary[word])\n",
    "\n",
    "            # Replace questions as word to question as number representation\n",
    "            dataset.set_value(index, question, question_to_number)\n",
    "\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embeddings[index] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
