{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove vectors...\n",
      "Done.\n",
      "Load tokenized train data...\n",
      "Done.\n",
      "Load tokenized test data...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Loading glove vectors...')\n",
    "glove_wiki = pd.read_pickle('./data/glove_wiki.pkl')\n",
    "print('Done.')\n",
    "\n",
    "print ('Load tokenized train data...')\n",
    "token_train = pd.read_pickle('./data/token_train.pkl')\n",
    "print('Done.')\n",
    "\n",
    "print ('Load tokenized test data...')\n",
    "token_test = pd.read_pickle('./data/token_test.pkl')\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Lookup\" words in tokenized questions from Glove_wiki "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still misspelled and uncommon words that could not be found though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look up words in Glove wiki...\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "There are 38876 missing words\n",
      "Those words are replaced with vectors of zeros\n",
      "Look up words in Glove wiki...\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "There are 35355 missing words\n",
      "Those words are replaced with vectors of zeros\n",
      "Saving vec_train to pickle...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "vec_train_glove = token_train[['id', 'qid1', 'qid2', 'is_duplicate']]\n",
    "\n",
    "vec_train_glove = vec_train_glove.assign(vecq1 = token_to_vec(token_train['tokenq1'], glove_wiki))\n",
    "vec_train_glove = vec_train_glove.assign(vecq2 = token_to_vec(token_train['tokenq2'], glove_wiki))\n",
    "\n",
    "print('Saving vec_train to pickle...')\n",
    "vec_train_glove.to_pickle(\"./data/vec_train_glove.pkl\")\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look up words in Glove wiki...\n",
      "0\n",
      "There are 9672 missing words\n",
      "Those words are replaced with vectors of zeros\n",
      "Look up words in Glove wiki...\n",
      "0\n",
      "There are 8883 missing words\n",
      "Those words are replaced with vectors of zeros\n",
      "Saving vec_test to pickle...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "vec_test_glove = token_test[['test_id', 'qid1', 'qid2']]\n",
    "\n",
    "vec_test_glove = vec_test_glove.assign(vecq1 = token_to_vec(token_test['tokenq1'], glove_wiki))\n",
    "vec_test_glove = vec_test_glove.assign(vecq2 = token_to_vec(token_test['tokenq2'], glove_wiki))\n",
    "\n",
    "print('Saving vec_test to pickle...')\n",
    "vec_test_glove.to_pickle(\"./data/vec_test_glove.pkl\")\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
